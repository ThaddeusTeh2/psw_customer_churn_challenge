{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0a24b5",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - @FCS11424\n",
    "\n",
    "# Repo: https://github.com/ThaddeusTeh2/psw_customer_churn_challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb1e5de",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "- Customer churn prediction is critical for business retention. \n",
    "- Current machine learning approaches often suffer from data leakage, \n",
    " leading to inflated performance metrics and unreliable predictions in a real-world setting. \n",
    "- This poses a challenge in developing an effective, deployable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70457d2",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "- The goal is to design, implement, and evaluate a machine learning pipeline that accurately predicts customer churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5c7e27",
   "metadata": {},
   "source": [
    "# Load the dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd379810",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cf00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load dat data\n",
    "df = pd.read_csv('./customer_churn.csv')\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ef036",
   "metadata": {},
   "source": [
    "# Lookie at the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a63a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed977d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553132ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "\n",
    "# dayum clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4429b",
   "metadata": {},
   "source": [
    "### the good stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = [\n",
    "    \"gender\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"PhoneService\",\n",
    "    \"MultipleLines\",\n",
    "    \"InternetService\",\n",
    "    \"OnlineSecurity\",\n",
    "    \"OnlineBackup\",\n",
    "    \"DeviceProtection\",\n",
    "    \"TechSupport\",\n",
    "    \"StreamingTV\",\n",
    "    \"StreamingMovies\",\n",
    "    \"Contract\",\n",
    "    \"PaperlessBilling\",\n",
    "    \"PaymentMethod\",\n",
    "    \"Churn\"\n",
    "]\n",
    "\n",
    "for col in cols_to_check:\n",
    "    if col in df.columns:\n",
    "        print(f\"Unique vals in '{col}':\")\n",
    "        print(df[col].unique())\n",
    "        print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72d2bf",
   "metadata": {},
   "source": [
    "# slight DF cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dont feel like i need these\n",
    "\n",
    "columns_to_drop = [\"customerID\", \"TotalCharges\"]\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028945f0",
   "metadata": {},
   "source": [
    "- TotalCharges is highly correlated with both MonthlyCharges and tenure.\n",
    "\n",
    "- TotalCharges is calculated as tenure multiplied by MonthlyCharges.\n",
    "\n",
    "- The presence of these three highly correlated variables can introduce multicollinearity into a machine learning model, which can make a model's coefficients unstable and difficult to interpret.\n",
    "\n",
    "- The decision to drop TotalCharges is a common practice in analyses of this specific dataset to mitigate multicollinearity while retaining the core information in the other two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5994a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df into categorical and numerical columns\n",
    "\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numerical columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11e792",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd40f1c",
   "metadata": {},
   "source": [
    "## any cols directly affect our target variable 'Churn'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa2b33",
   "metadata": {},
   "source": [
    "### categorical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af751f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All categorical features vs Churn\n",
    "for feature in categorical_cols:\n",
    "    if feature == 'Churn':  # skip target itself\n",
    "        continue\n",
    "    plt.figure(figsize=(10,6))\n",
    "    churn_counts = pd.crosstab(df[feature], df['Churn'], normalize='index') * 100\n",
    "    churn_counts.plot(kind='bar', stacked=True, color=['skyblue','salmon'], figsize=(10,6))\n",
    "    plt.title(f'{feature} vs Churn (%)')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.legend(title='Churn', loc='upper right')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565651f",
   "metadata": {},
   "source": [
    "#### sanity checkplot: churn distributionm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b65bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "df['Churn'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['skyblue','salmon'], startangle=90)\n",
    "plt.title('Churn Distribution')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f4d4a",
   "metadata": {},
   "source": [
    "#### binary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define binary categorical features\n",
    "binary_cats = [\n",
    "    'Partner', 'Dependents', 'PhoneService',\n",
    "    'PaperlessBilling', 'SeniorCitizen'\n",
    "]\n",
    "\n",
    "# Binary features vs Churn\n",
    "for feature in binary_cats:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(data=df, x=feature, y=df['Churn'].map({'No':0,'Yes':1}), palette='Set2')\n",
    "    plt.title(f'Churn Rate by {feature}')\n",
    "    plt.ylabel('Churn Rate')\n",
    "    plt.xlabel(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6773e0b",
   "metadata": {},
   "source": [
    "### numerical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49774e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features vs Churn\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(data=df, x='Churn', y=col, palette='Set2')\n",
    "    sns.stripplot(data=df, x='Churn', y=col, color='black', alpha=0.3, jitter=True)\n",
    "    plt.title(f'{col} distribution by Churn')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f06f9",
   "metadata": {},
   "source": [
    "#### heatmap of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ed677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "num_summary = df[numerical_cols].describe().T  # count, mean, std, min, 25%, 50%, 75%, max\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(num_summary, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Summary Statistics of Numerical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67868839",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family indicator (Partner + Dependents)\n",
    "df['Family'] = ((df['Partner'] == 'Yes') | (df['Dependents'] == 'Yes')).astype(int)\n",
    "# Captures household/family support as a stabilizing factor against churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffa90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Services count (how many optional services are active)\n",
    "service_cols = ['OnlineSecurity','OnlineBackup','DeviceProtection',\n",
    "                'TechSupport','StreamingTV','StreamingMovies']\n",
    "df['ServicesCount'] = df[service_cols].apply(lambda row: (row == 'Yes').sum(), axis=1)\n",
    "# Higher service engagement usually correlates with lower churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phone service merged into a single scale\n",
    "def phone_lines(row):\n",
    "    if row['PhoneService'] == 'No':\n",
    "        return 0\n",
    "    elif row['MultipleLines'] == 'Yes':\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "df['PhoneLines'] = df.apply(phone_lines, axis=1)\n",
    "# Simplifies PhoneService + MultipleLines into one ordinal feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LongTermContract flag\n",
    "df['LongTermContract'] = df['Contract'].apply(lambda x: 1 if x in ['One year','Two year'] else 0)\n",
    "# Longer commitments generally reduce churn likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af000760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutomaticPayment flag\n",
    "df['AutoPayment'] = df['PaymentMethod'].apply(lambda x: 1 if 'automatic' in x.lower() else 0)\n",
    "# Automatic payments usually indicate stability and lower churn risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenure group buckets\n",
    "df['TenureGroup'] = pd.cut(\n",
    "    df['tenure'],\n",
    "    bins=[0,12,24,48,72,float('inf')],\n",
    "    labels=['0-12','12-24','24-48','48-72','72+'],\n",
    "    include_lowest=True\n",
    ")\n",
    "# Groups customers by service length to capture lifecycle effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53267c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High spender binary feature (top 25% of monthly charges)\n",
    "threshold = df['MonthlyCharges'].quantile(0.75)\n",
    "df['HighSpender'] = (df['MonthlyCharges'] > threshold).astype(int)\n",
    "# Flags customers paying in the top quartile of charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent divide by zero\n",
    "df['SpendPerService'] = df.apply(lambda row: row['MonthlyCharges'] / row['ServicesCount'] if row['ServicesCount'] > 0 else 0, axis=1)\n",
    "# Normalizes spending intensity per subscribed service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965770d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Interaction: Contract x PaymentMethod\n",
    "df['Contract_Payment'] = df['Contract'] + '_' + df['PaymentMethod']\n",
    "# Captures risky combinations like month-to-month + electronic check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fb9b7",
   "metadata": {},
   "source": [
    "# save and switch to new dataset with feature engineered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8107ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('customer_churn_w_fe_cols.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c70b4",
   "metadata": {},
   "source": [
    "# use the new df with fe cols as df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dat data\n",
    "df2 = pd.read_csv('./customer_churn_w_fe_cols.csv')\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d97f01",
   "metadata": {},
   "source": [
    "## what df look like after FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a56e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24882663",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = [\n",
    "    \"gender\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"Partner\",\n",
    "    \"Dependents\",\n",
    "    \"PhoneService\",\n",
    "    \"MultipleLines\",\n",
    "    \"InternetService\",\n",
    "    \"OnlineSecurity\",\n",
    "    \"OnlineBackup\",\n",
    "    \"DeviceProtection\",\n",
    "    \"TechSupport\",\n",
    "    \"StreamingTV\",\n",
    "    \"StreamingMovies\",\n",
    "    \"Contract\",\n",
    "    \"PaperlessBilling\",\n",
    "    \"PaymentMethod\",\n",
    "    \"Churn\",\n",
    "    \"Family\",\n",
    "    \"PhoneLines\",\n",
    "    \"LongTermContract\",\n",
    "    \"AutoPayment\",\n",
    "    \"TenureGroup\",\n",
    "    \"HighSpender\",\n",
    "    \"Contract_Payment\"\n",
    "]\n",
    "\n",
    "for col in cols_to_check:\n",
    "    if col in df2.columns:\n",
    "        print(f\"Unique vals in '{col}':\")\n",
    "        print(df2[col].unique())\n",
    "        print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d95926f",
   "metadata": {},
   "source": [
    "# encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cded69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "\n",
    "for col in df2:\n",
    "    df2[col+'_encoded'] = label.fit_transform(df2[col])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df2.select_dtypes(['int64','float64'])\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945633a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae37432",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['SeniorCitizen', 'tenure', 'MonthlyCharges', 'Family', 'ServicesCount', 'PhoneLines', 'LongTermContract', 'AutoPayment', 'HighSpender', 'SpendPerService'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd498f6",
   "metadata": {},
   "source": [
    "### what the data look like after encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4fa76f",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756c5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, roc_curve, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fff277",
   "metadata": {},
   "source": [
    "### X,Y train test split\n",
    "\n",
    "#### KPIs_met_more_than_80 as target = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = final_df.drop(['Churn_encoded'], axis=1)\n",
    "y = final_df.Churn_encoded\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb47e45",
   "metadata": {},
   "source": [
    "### check the class balance (how many ppl churn(1) vs not(0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658dc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y)\n",
    "plt.title('Class Balance of Churn')\n",
    "plt.xlabel('Churn (encoded)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f052ab8",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d98c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (throw different models in and see which one performs best)\n",
    "\n",
    "# dont have to use this many, just pick the best\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "# only for XGBclassifier, to handle class imbalance\n",
    "neg, pos = np.bincount(y_train)\n",
    "#scale_pos_weight = (number of negative samples) / (number of positive samples)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Ridge Classifier': RidgeClassifier(random_state=42),\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, scale_pos_weight=scale_pos_weight),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        # For models like RidgeClassifier, use decision_function and sigmoid\n",
    "        from scipy.special import expit\n",
    "        y_proba = expit(model.decision_function(X_test_scaled))\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15597d34",
   "metadata": {},
   "source": [
    "# cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42, class_weight=\"balanced\"),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Define CV and scoring metrics\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "# Run CV for each model\n",
    "cv_results = []\n",
    "for name, model in models.items():\n",
    "    scores = cross_validate(model, X_train_scaled, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    cv_results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": scores['test_accuracy'].mean(),\n",
    "        \"precision\": scores['test_precision'].mean(),\n",
    "        \"recall\": scores['test_recall'].mean(),\n",
    "        \"f1_score\": scores['test_f1'].mean(),\n",
    "        \"roc_auc\": scores['test_roc_auc'].mean()\n",
    "    })\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc93e36",
   "metadata": {},
   "source": [
    "# model justification\n",
    "\n",
    "- Logistic Regression: Lower accuracy, but recall is high (+- 0.79) >> better at catching churners, though at cost of more false alarms\n",
    "- Gradient Boosting: Higher accuracy, precision, and ROC-AUC >> overall stronger balanced model, but recall is lower (+- 0.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Logistic Regression pipeline\n",
    "logreg_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(\n",
    "        solver='liblinear',        # works with l1 and l2\n",
    "        class_weight='balanced',   # handle imbalance\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10],         # regularization strength\n",
    "    'logreg__penalty': ['l1', 'l2']          # type of regularization\n",
    "}\n",
    "\n",
    "# GridSearchCV with recall focus\n",
    "grid = GridSearchCV(\n",
    "    logreg_pipe,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='recall',   # focus on catching churners\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best mean recall (CV):\", grid.best_score_)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred_logreg = grid.predict(X_test)\n",
    "y_proba_logreg = grid.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcb8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_logreg), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_logreg)\n",
    "auc_score = roc_auc_score(y_test, y_proba_logreg)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Random guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742b59d",
   "metadata": {},
   "source": [
    "# Customer groups at highest churn risk\n",
    "Based on feature engineering + literature on this dataset, the high-risk groups are:\n",
    "- Contract = Month-to-month > highest churn.\n",
    "- PaymentMethod = Electronic check > disproportionately churn-prone.\n",
    "- Low tenure (0–12 months) > customers who haven’t built loyalty yet.\n",
    "- Few or no extra services (low ServicesCount) > less “stickiness” to the company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020033b2",
   "metadata": {},
   "source": [
    "# extract coefficients as odds ratios from tuned Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de753b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best estimator from GridSearch\n",
    "best_logreg = grid.best_estimator_.named_steps['logreg']\n",
    "\n",
    "# Feature names (after scaling/encoding)\n",
    "feature_names = X_train.columns  # adjust if using encoded/expanded features\n",
    "\n",
    "# Coefficients → odds ratios\n",
    "odds_ratios = pd.Series(\n",
    "    np.exp(best_logreg.coef_[0]), \n",
    "    index=feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "# Display top 15 churn-increasing features\n",
    "odds_ratios.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60618e",
   "metadata": {},
   "source": [
    "-\tValues > 1 → increase churn odds.\n",
    "-\tValues < 1 → reduce churn odds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847c9b8",
   "metadata": {},
   "source": [
    "# Actionable Insights\n",
    "1.\tMonth-to-Month Contracts (highest churn risk)\n",
    "\t-\tCustomers on short-term contracts are unstable.\n",
    "\t-\tAction: Offer bundled discounts for upgrading to annual contracts; implement loyalty rewards after 6 months to encourage long-term retention.\n",
    "2.\tElectronic Check Payments (high churn group)\n",
    "\t-\tThis group shows poor retention compared to auto-payment customers.\n",
    "\t-\tAction: Provide small bill credits or convenience perks for switching to automatic payment methods (bank transfer, credit card).\n",
    "3.\tLow Tenure (0–12 months)\n",
    "\t-\tNew customers are far more likely to churn before establishing habits.\n",
    "\t-\tAction: Implement a structured onboarding program (welcome discounts, check-ins, tutorials) to increase stickiness in the first year.\n",
    "4.\tLow ServicesCount (few subscribed services)\n",
    "\t-\tCustomers with only 1–2 services are less tied to the provider.\n",
    "\t-\tAction: Run cross-sell campaigns — offer attractive bundle pricing (e.g., Internet + Streaming) to deepen engagement and make churn less likely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b394bb9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
